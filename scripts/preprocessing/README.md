# Preprocessing
This directory contains the scripts used to preprocess the RNA-seq data that was used for training and testing the brain clock model.
## Note on two-step training
As indicated in the **Methods** and in the workflow depicted in the [**root directory**](./) of this repository, our model was trained in two different steps: 
1. **First step:**
    - Train model on preprocessed matrix with all the genes common accross the integrated datasets.
    - Identified 664 genes with non-zero coefficients
2. **Second step:**
    - Train model on a matrix that, prior to being preprocessed, was filtered to keep only the 664 genes selected by the first fitting (indicated in the file `modFuncsAlpha1_coefs.csv`). 
    - Perform second training round on the filtered preprocessed matrix, which selected 431 genes among the 664 initially provided.
Therefore, two slightly different preprocessing pipelines are performed by the scripts included in this directory depending on if it's for first or second round of training. They are coordinated by two different bash scripts and can be executed as follows:
1. **Preprocessing for first training round**
```bash
sbatch launch_preproc_pipe.sh
```
2. **Preprocessing for first training round**
```bash
sbatch launch_preproc_pipe_filtsigngenes.sh
```
### Outcomes of the two-step training process
- The final model is 35% smaller, with a only a slight decline in predictive performance.
- The 664 genes obtained from the first step were used to derive surrogate variables, enabling the training of a frozen SVA model. This model corrects inferred surrogate variables based on the expression of these 664 genes, and is implemented in our package [`brainAgeShiftR`](https://gitlab.lcsb.uni.lu/CBG/brainAgeShiftR) via the function:
```r
do_frozenSVA()
```
## Input files
The preprocessing pipeline requires the following input files:
- `results/parsed/merged/merged_counts.csv`: the merged counts of the clinical datasets, generated by [dataset_parsing](./scripts/dataset_parsing/) module (`quant_norm(log2(counts + 1))`), where genes are in columns and samples are in rows.
- `results/parsed/merged_metdat.csv`: a metadata file of the clinical samples.
- `results/models/first_round/mod_alpha1_coefs.csv`: a file with the genes and coefficients obtained after the first round of training. This file is only necessary for the preprocessing that is part of the second round of training, which, as previously outlined, is executed by `launch_preproc_pipe_filtsigngenes.sh`.

If previous steps ([dataset_parsing](./scripts/dataset_parsing) and [model_training](./scripts/model_training), for round 2) ran without errors, **the required files should be already located in the corresponding directories**.

The `data.csv` dataset includes samples from multiple sources: 
- [**The RNAseq Harmonization Study (rnaSeqReprocessing)**](https://adknowledgeportal.synapse.org/Explore/Studies/DetailsPage/StudyDetails?Study=syn9702085)
- [**The Living Brain Project (LBP)**](https://adknowledgeportal.synapse.org/Explore/Studies/DetailsPage/StudyDetails?Study=syn26337520)
- [**The Aging, Dementia and Traumatic Brain Injury Study(TBI)**](https://aging.brain-map.org)
- [**The Genotype-Tissue Expression (GTEx) project (GTEx)**](https://www.gtexportal.org/home/downloads/adultgtex/bulk_tissue_expression)
- Pseudobulk data of brain samples from [**ageAnno**](https://relab.xidian.edu.cn/AgeAnno/#/)
- Brain cell types controls and perturbation data from Level 3 of [**LINCS L1000**](https://maayanlab.cloud/sigcom-lincs/#/Download)
- A compilation of perturbation datasets collected from several sources (indicated in the Table S5)
This matrix was generated in the **module 1 (Dataset parsing)**, which handled some preprocessing required for datasets integration.
## Preprocessing steps
The following steps are performed in the preprocessing pipelines:
1. **Filtering of selected genes**: only in the preprocessing for the second round of training, performed by `launch_preproc_pipe_filtsigngenes.sh`.
2. **Principal component analysis (PCA):** assess batch effects and identify potential sub-tissue differences in brain samples.
3. **Cerebellum sample removal:** Exclude cerebellum samples due to their distinct transcriptional profiles.
4. **PCA re-evaluation:** re-run PCA after cerebellum sample removal.
5. **Batch effect correction:** Apply **surrogate variable analysis (SVA)** to remove batch effects by regressing out the surrogate variables.
6. **Post-SVA PCA:** perform PCA again to confiorm batch structure removal.
7. **Final surrogate variable computation:** compute surrogate variables on the SVA-adjusted dataset and visualize them to ensure batch effects are eliminated.